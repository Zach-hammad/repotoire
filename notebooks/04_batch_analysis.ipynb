{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Analysis of Multiple Codebases\n",
    "\n",
    "This notebook demonstrates how to analyze multiple projects and compare their health metrics.\n",
    "\n",
    "Topics covered:\n",
    "1. Analyzing multiple projects\n",
    "2. Comparing health scores\n",
    "3. Identifying patterns across projects\n",
    "4. Generating comparative reports\n",
    "5. Tracking metrics over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from falkor.graph import Neo4jClient\n",
    "from falkor.pipeline import IngestionPipeline\n",
    "from falkor.detectors import AnalysisEngine\n",
    "from falkor.config import load_config\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Projects to Analyze\n",
    "\n",
    "List the repositories you want to analyze and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define projects\n",
    "projects = [\n",
    "    {\n",
    "        \"name\": \"Project A\",\n",
    "        \"path\": \"/path/to/project-a\",\n",
    "        \"patterns\": [\"**/*.py\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Project B\",\n",
    "        \"path\": \"/path/to/project-b\",\n",
    "        \"patterns\": [\"**/*.py\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Project C\",\n",
    "        \"path\": \"/path/to/project-c\",\n",
    "        \"patterns\": [\"**/*.py\", \"**/*.js\"]\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Configured {len(projects)} projects for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyze Each Project\n",
    "\n",
    "Run the complete analysis workflow for each project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_project(project_config, config):\n",
    "    \"\"\"\n",
    "    Analyze a single project and return health report.\n",
    "    \"\"\"\n",
    "    name = project_config[\"name\"]\n",
    "    repo_path = project_config[\"path\"]\n",
    "    patterns = project_config[\"patterns\"]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Analyzing: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Connect to Neo4j\n",
    "    db = Neo4jClient(\n",
    "        uri=config.neo4j.uri,\n",
    "        username=config.neo4j.user,\n",
    "        password=config.neo4j.password\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Clear previous data for this project (optional)\n",
    "        # db.clear_graph()  # WARNING: Clears entire graph!\n",
    "        \n",
    "        # Ingest\n",
    "        print(f\"\\n1. Ingesting {name}...\")\n",
    "        pipeline = IngestionPipeline(\n",
    "            repo_path=repo_path,\n",
    "            neo4j_client=db,\n",
    "            follow_symlinks=False,\n",
    "            max_file_size_mb=10,\n",
    "            batch_size=100\n",
    "        )\n",
    "        pipeline.ingest(patterns=patterns)\n",
    "        \n",
    "        stats = db.get_stats()\n",
    "        print(f\"   Files: {stats['total_files']}\")\n",
    "        print(f\"   Classes: {stats['total_classes']}\")\n",
    "        print(f\"   Functions: {stats['total_functions']}\")\n",
    "        \n",
    "        # Analyze\n",
    "        print(f\"\\n2. Running analysis...\")\n",
    "        engine = AnalysisEngine(db)\n",
    "        health = engine.analyze()\n",
    "        \n",
    "        print(f\"   Grade: {health.grade}\")\n",
    "        print(f\"   Score: {health.overall_score:.1f}/100\")\n",
    "        print(f\"   Findings: {health.findings_summary.total}\")\n",
    "        \n",
    "        # Save report\n",
    "        output_file = f\"reports/{name.replace(' ', '_').lower()}_report.json\"\n",
    "        Path(\"reports\").mkdir(exist_ok=True)\n",
    "        with open(output_file, \"w\") as f:\n",
    "            report_data = health.to_dict()\n",
    "            report_data[\"project_name\"] = name\n",
    "            report_data[\"project_path\"] = repo_path\n",
    "            json.dump(report_data, f, indent=2)\n",
    "        \n",
    "        print(f\"   Report saved: {output_file}\")\n",
    "        \n",
    "        return {\n",
    "            \"name\": name,\n",
    "            \"health\": health,\n",
    "            \"stats\": stats,\n",
    "            \"report_file\": output_file\n",
    "        }\n",
    "        \n",
    "    finally:\n",
    "        db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = load_config()\n",
    "\n",
    "# Analyze all projects\n",
    "results = []\n",
    "for project in projects:\n",
    "    try:\n",
    "        result = analyze_project(project, config)\n",
    "        results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error analyzing {project['name']}: {e}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"‚úì Analysis complete for {len(results)}/{len(projects)} projects\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare Health Scores\n",
    "\n",
    "Create a comparative view of all projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build comparison dataframe\n",
    "comparison_data = []\n",
    "for result in results:\n",
    "    health = result[\"health\"]\n",
    "    stats = result[\"stats\"]\n",
    "    \n",
    "    comparison_data.append({\n",
    "        \"Project\": result[\"name\"],\n",
    "        \"Grade\": health.grade,\n",
    "        \"Overall Score\": round(health.overall_score, 1),\n",
    "        \"Structure\": round(health.structure_score, 1),\n",
    "        \"Quality\": round(health.quality_score, 1),\n",
    "        \"Architecture\": round(health.architecture_score, 1),\n",
    "        \"Files\": stats[\"total_files\"],\n",
    "        \"Classes\": stats[\"total_classes\"],\n",
    "        \"Functions\": stats[\"total_functions\"],\n",
    "        \"Critical\": health.findings_summary.critical,\n",
    "        \"High\": health.findings_summary.high,\n",
    "        \"Medium\": health.findings_summary.medium\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nüìä Project Comparison:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Save comparison\n",
    "df.to_csv(\"reports/comparison.csv\", index=False)\n",
    "print(\"\\n‚úì Saved to reports/comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score comparison chart\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Overall scores\n",
    "axes[0, 0].barh(df[\"Project\"], df[\"Overall Score\"], color='skyblue')\n",
    "axes[0, 0].set_xlabel('Score')\n",
    "axes[0, 0].set_title('Overall Health Score')\n",
    "axes[0, 0].set_xlim([0, 100])\n",
    "\n",
    "# Category scores\n",
    "x = range(len(df))\n",
    "width = 0.25\n",
    "axes[0, 1].bar([i - width for i in x], df[\"Structure\"], width, label='Structure', color='lightgreen')\n",
    "axes[0, 1].bar(x, df[\"Quality\"], width, label='Quality', color='lightcoral')\n",
    "axes[0, 1].bar([i + width for i in x], df[\"Architecture\"], width, label='Architecture', color='lightyellow')\n",
    "axes[0, 1].set_ylabel('Score')\n",
    "axes[0, 1].set_title('Category Scores')\n",
    "axes[0, 1].set_xticks(x)\n",
    "axes[0, 1].set_xticklabels(df[\"Project\"], rotation=45, ha='right')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].set_ylim([0, 100])\n",
    "\n",
    "# Codebase size\n",
    "axes[1, 0].bar(df[\"Project\"], df[\"Files\"], color='mediumpurple')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].set_title('Number of Files')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Findings\n",
    "axes[1, 1].bar(df[\"Project\"], df[\"Critical\"], label='Critical', color='red')\n",
    "axes[1, 1].bar(df[\"Project\"], df[\"High\"], bottom=df[\"Critical\"], label='High', color='orange')\n",
    "axes[1, 1].bar(df[\"Project\"], df[\"Medium\"], bottom=df[\"Critical\"] + df[\"High\"], label='Medium', color='yellow')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "axes[1, 1].set_title('Findings by Severity')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('reports/comparison_charts.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Charts saved to reports/comparison_charts.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Identify Best and Worst Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best project\n",
    "best_project = df.loc[df[\"Overall Score\"].idxmax()]\n",
    "print(\"\\nüèÜ Best Project:\")\n",
    "print(f\"   {best_project['Project']} - Grade {best_project['Grade']} ({best_project['Overall Score']}/100)\")\n",
    "\n",
    "# Worst project\n",
    "worst_project = df.loc[df[\"Overall Score\"].idxmin()]\n",
    "print(\"\\n‚ö†Ô∏è  Needs Attention:\")\n",
    "print(f\"   {worst_project['Project']} - Grade {worst_project['Grade']} ({worst_project['Overall Score']}/100)\")\n",
    "\n",
    "# Project with most issues\n",
    "df[\"Total Issues\"] = df[\"Critical\"] + df[\"High\"] + df[\"Medium\"]\n",
    "most_issues = df.loc[df[\"Total Issues\"].idxmax()]\n",
    "print(\"\\nüîß Most Issues:\")\n",
    "print(f\"   {most_issues['Project']} - {most_issues['Total Issues']} issues ({most_issues['Critical']} critical)\")\n",
    "\n",
    "# Averages\n",
    "print(\"\\nüìà Average Metrics:\")\n",
    "print(f\"   Overall Score: {df['Overall Score'].mean():.1f}\")\n",
    "print(f\"   Files per project: {df['Files'].mean():.0f}\")\n",
    "print(f\"   Issues per project: {df['Total Issues'].mean():.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Executive Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create executive summary\n",
    "summary = {\n",
    "    \"analysis_date\": datetime.now().isoformat(),\n",
    "    \"projects_analyzed\": len(results),\n",
    "    \"summary\": {\n",
    "        \"average_score\": round(df[\"Overall Score\"].mean(), 1),\n",
    "        \"grade_distribution\": df[\"Grade\"].value_counts().to_dict(),\n",
    "        \"total_files\": int(df[\"Files\"].sum()),\n",
    "        \"total_classes\": int(df[\"Classes\"].sum()),\n",
    "        \"total_functions\": int(df[\"Functions\"].sum()),\n",
    "        \"total_critical_issues\": int(df[\"Critical\"].sum()),\n",
    "        \"total_high_issues\": int(df[\"High\"].sum())\n",
    "    },\n",
    "    \"best_project\": {\n",
    "        \"name\": best_project[\"Project\"],\n",
    "        \"grade\": best_project[\"Grade\"],\n",
    "        \"score\": best_project[\"Overall Score\"]\n",
    "    },\n",
    "    \"needs_attention\": {\n",
    "        \"name\": worst_project[\"Project\"],\n",
    "        \"grade\": worst_project[\"Grade\"],\n",
    "        \"score\": worst_project[\"Overall Score\"],\n",
    "        \"critical_issues\": worst_project[\"Critical\"]\n",
    "    },\n",
    "    \"projects\": comparison_data\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "with open(\"reports/executive_summary.json\", \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\n‚úì Executive summary saved to reports/executive_summary.json\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXECUTIVE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(json.dumps(summary, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Track Metrics Over Time (Optional)\n",
    "\n",
    "Save results with timestamps to track improvements over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append to historical data\n",
    "history_file = \"reports/history.jsonl\"\n",
    "\n",
    "with open(history_file, \"a\") as f:\n",
    "    for project_data in comparison_data:\n",
    "        record = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            **project_data\n",
    "        }\n",
    "        f.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "print(f\"‚úì Historical data appended to {history_file}\")\n",
    "print(\"\\nRun this analysis regularly to track progress over time!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated how to:\n",
    "\n",
    "1. **Batch Analysis**: Analyze multiple projects efficiently\n",
    "2. **Comparison**: Compare health scores and metrics\n",
    "3. **Patterns**: Identify common issues across projects\n",
    "4. **Reporting**: Generate executive summaries\n",
    "5. **Tracking**: Monitor improvements over time\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "- **Portfolio Management**: Track health of all company projects\n",
    "- **Team Comparisons**: Compare team code quality\n",
    "- **Migration Planning**: Identify which projects need refactoring first\n",
    "- **Progress Tracking**: Monitor improvements after refactoring initiatives\n",
    "- **Best Practices**: Learn from highest-quality projects\n",
    "\n",
    "## Tips for Production Use\n",
    "\n",
    "1. Run analysis on a schedule (daily/weekly)\n",
    "2. Store results in a time-series database\n",
    "3. Set up alerts for critical issues\n",
    "4. Create dashboards for stakeholders\n",
    "5. Integrate with CI/CD pipelines\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Automate with a cron job or CI/CD pipeline\n",
    "- Create custom dashboards with your favorite BI tool\n",
    "- Set quality gates based on scores\n",
    "- Share reports with stakeholders"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
